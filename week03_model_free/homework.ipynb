{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gOfEF-y7uU21",
        "outputId": "d2912c41-5e5a-4ded-855a-6a7bd460b984"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q gymnasium[classic-control]\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\"\n",
        "\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({\"x\": np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "\n",
        "def seed_everything(env, seed=None):\n",
        "    if seed is None:\n",
        "        seed = SEED\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "\n",
        "def visualize_agent(env, agent, max_steps=100, delay=0.1):\n",
        "    \"\"\"\n",
        "    Visualize the agent's behavior in the environment.\n",
        "\n",
        "    Args:\n",
        "        env: The environment\n",
        "        agent: The trained agent\n",
        "        max_steps: Maximum number of steps to take\n",
        "        delay: Time delay between steps for visualization\n",
        "    \"\"\"\n",
        "    s, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Render the environment\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(env.render())\n",
        "        plt.title(f\"Step: {step}, Total Reward: {total_reward:.2f}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        # Get action from the agent\n",
        "        a = agent.get_best_action(s)  # Use best action for visualization\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_s, r, done, _, _ = env.step(a)\n",
        "\n",
        "        # Update state and reward\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "\n",
        "        # Add delay for better visualization\n",
        "        time.sleep(delay)\n",
        "\n",
        "        if done:\n",
        "            # Show final state\n",
        "            clear_output(True)\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(env.render())\n",
        "            plt.title(f\"Final State - Steps: {step + 1}, Total Reward: {total_reward:.2f}\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "            break\n",
        "\n",
        "\n",
        "def benchmark_agents(\n",
        "    exp_setups,\n",
        "    num_episodes=1000,\n",
        "    plot_every=100,\n",
        "    t_max=10000,\n",
        "    span=100,\n",
        "    patch_every=None,\n",
        "    patch_foo=None,\n",
        "    num_seeds=3,\n",
        "):\n",
        "    all_rewards = {}\n",
        "    envs = {exp_setup[\"name\"]: exp_setup[\"env\"]() for exp_setup in exp_setups}\n",
        "    agents_buiders = {exp_setup[\"name\"]: exp_setup[\"agent_builder\"] for exp_setup in exp_setups}\n",
        "    train_foo = {exp_setup[\"name\"]: exp_setup[\"train_foo\"] for exp_setup in exp_setups}\n",
        "\n",
        "    for seed in range(num_seeds):\n",
        "        SEED = seed + 42  # Using different seeds\n",
        "        agents = {agent_name: agent() for agent_name, agent in agents_buiders.items()}\n",
        "\n",
        "        # Create a separate environment for each agent using the env function\n",
        "        for agent_name, agent in agents.items():\n",
        "            agents[agent_name].env = envs[agent_name]\n",
        "\n",
        "        seed_rewards = {agent_name: [] for agent_name in agents_buiders}\n",
        "\n",
        "        # Seed each environment separately\n",
        "        for agent_name in agents:\n",
        "            seed_everything(envs[agent_name], seed=SEED)\n",
        "\n",
        "        tbar = trange(num_episodes)\n",
        "        tbar.set_description(f\"Seed {seed + 1}/{num_seeds}\")\n",
        "        for i in tbar:\n",
        "            for agent_name, agent in agents.items():\n",
        "                seed_rewards[agent_name].append(train_foo[agent_name](envs[agent_name], agent))\n",
        "            if i % 10 == 0:\n",
        "                tbar.set_postfix({agent_name: seed_rewards[agent_name][-1] for agent_name in agents}, refresh=True)\n",
        "\n",
        "        # Store rewards for this seed\n",
        "        for agent_name, rewards_list in seed_rewards.items():\n",
        "            if agent_name not in all_rewards:\n",
        "                all_rewards[agent_name] = []\n",
        "            all_rewards[agent_name].append(rewards_list)\n",
        "\n",
        "        # Average rewards across seeds\n",
        "        avg_rewards = {\n",
        "            agent_name: np.mean(np.array(seed_results), axis=0) for agent_name, seed_results in all_rewards.items()\n",
        "        }\n",
        "\n",
        "        # Calculate standard deviation for confidence intervals\n",
        "        std_rewards = {\n",
        "            agent_name: np.std(np.array(seed_results), axis=0) for agent_name, seed_results in all_rewards.items()\n",
        "        }\n",
        "\n",
        "        # Plot average performance across seeds with confidence tubes\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for agent_name, rewards_list in avg_rewards.items():\n",
        "            mean_rewards = moving_average(rewards_list, span=span)\n",
        "            std_rewards_smoothed = moving_average(std_rewards[agent_name], span=span)\n",
        "\n",
        "            # Plot mean line\n",
        "            plt.plot(mean_rewards, label=f\"{agent_name} (avg of {num_seeds} seeds)\")\n",
        "\n",
        "            # Plot confidence tubes (mean ± std)\n",
        "            plt.fill_between(\n",
        "                range(len(mean_rewards)),\n",
        "                mean_rewards - std_rewards_smoothed,\n",
        "                mean_rewards + std_rewards_smoothed,\n",
        "                alpha=0.2,\n",
        "            )\n",
        "\n",
        "            # Draw solid contour lines for the confidence tube borders\n",
        "            plt.plot(range(len(mean_rewards)), mean_rewards - std_rewards_smoothed, \"--\", color=\"gray\", alpha=0.7)\n",
        "            plt.plot(range(len(mean_rewards)), mean_rewards + std_rewards_smoothed, \"--\", color=\"gray\", alpha=0.7)\n",
        "\n",
        "        plt.title(\n",
        "            f\"{envs[list(envs.keys())[0]].spec.id} - Average performance across {num_seeds} seeds with confidence intervals\"\n",
        "        )\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    return avg_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTxm2NYHuU3B"
      },
      "source": [
        "## Seminar: Q-learning\n",
        "\n",
        "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
        "\n",
        "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below.\n",
        "\n",
        "**Tip**: To ensure reproducibility, use the environment’s built-in random generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-Xr1yY5IuU3G"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, env):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly.\n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.env = env\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_legal_actions(self, _state):\n",
        "        return list(range(self.env.action_space.n))\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\"Returns Q(state,action)\"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\"Sets the Qvalue for [state,action] to the given value\"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    # ---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        \n",
        "        value = np.max([self.get_qvalue(state, action) \n",
        "                       for action in possible_actions])\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        new_q_value = (1-learning_rate) * self.get_qvalue(state, action) + learning_rate*(reward + gamma*self.get_value(next_state))\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        self.set_qvalue(state, action, new_q_value)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values).\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        best_action = possible_actions[np.argmax([self.get_qvalue(state, action) for action in possible_actions])]\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.\n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.get_best_action).\n",
        "\n",
        "        Note: To pick randomly from a list, use random.choice(list).\n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        # Tip: Use self.env.np_random.random() to generate a random number\n",
        "        # <YOUR CODE HERE>\n",
        "        chosen_action = self.get_best_action(state) if self.env.np_random.random() > epsilon else self.env.np_random.choice(possible_actions)\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return chosen_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef5Gb93duU3I"
      },
      "source": [
        "### Try it on taxi\n",
        "\n",
        "Here we use the Q-Learning agent on the Taxi-v3 environment from OpenAI gym.\n",
        "You will need to complete a few of its functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6idEyRzvuU3J"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB7IekvOuU3K",
        "outputId": "c291f7ba-18ac-4f45-ba97-062b517effa8"
      },
      "outputs": [],
      "source": [
        "s, _ = env.reset(seed=SEED)\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_-aGIX-uU3L"
      },
      "outputs": [],
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should\n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        # <YOUR CODE HERE>\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated\n",
        "        # train (update) agent for state s\n",
        "        # <YOUR CODE HERE>\n",
        "        agent.update(s, a, r, next_s)\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wu1jqlruU3N",
        "outputId": "aef98649-33e5-48b1-c66b-8980d54a522c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "\n",
        "rewards = []\n",
        "seed_everything(env)\n",
        "\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.title(\"eps = {:e}, mean reward = {:.1f}\".format(agent.epsilon, np.mean(rewards[-10:])))\n",
        "        plt.plot(rewards)\n",
        "        plt.plot(moving_average(rewards))\n",
        "        plt.show()\n",
        "\n",
        "assert env.unwrapped.spec.id == \"Taxi-v3\" and np.mean(rewards[-100:]) >= 4.5, (\n",
        "    \"Please make sure that your agent is able to learn the optimal policy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04skwQAluU3P"
      },
      "source": [
        "# Seminar: Discretized state spaces\n",
        "\n",
        "Use agent to train efficiently on `CartPole-v0`. This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
        "\n",
        "The simplest way is to use `round(x, n_digits)` (or `np.round`) to round a real number to a given amount of digits. The tricky part is to get the `n_digits` right for each state to train effectively.\n",
        "\n",
        "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9cUU3lDuU3Q",
        "outputId": "7d949c8e-3be5-4168-dc3c-9ad3f302890d"
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    return gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env  # .env unwraps the TimeLimit wrapper\n",
        "\n",
        "\n",
        "env = make_env()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"first state: %s\" % (env.reset()[0]))\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxTsAsdpuU3R"
      },
      "source": [
        "### Play a few games\n",
        "\n",
        "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3qS_CR4uU3R"
      },
      "outputs": [],
      "source": [
        "def visualize_cartpole_observation_distribution(seen_observations):\n",
        "    seen_observations = np.array(seen_observations)\n",
        "\n",
        "    # The meaning of the observations is documented in\n",
        "    # https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
        "\n",
        "    # Get the number of dimensions from the state\n",
        "    n_dims = seen_observations.shape[1]\n",
        "\n",
        "    f, axarr = plt.subplots(1, n_dims, figsize=(16, 4), sharey=True)\n",
        "    titles = [\"Cart Position\", \"Cart Velocity\", \"Pole Angle\", \"Pole Velocity At Tip\"]\n",
        "\n",
        "    for i in range(n_dims):\n",
        "        ax = axarr[i]\n",
        "        ax.hist(seen_observations[:, i], bins=20)\n",
        "        ax.set_title(titles[i])\n",
        "        xmin, xmax = ax.get_xlim()\n",
        "        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n",
        "        ax.grid()\n",
        "    f.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPqjX6cXuU3S",
        "outputId": "9712d187-8811-4f3d-b60d-dcb61d25632e"
      },
      "outputs": [],
      "source": [
        "def gather_samples(env, max_steps=100000):\n",
        "    seen_observations = []\n",
        "    total_steps = 0\n",
        "\n",
        "    while total_steps < max_steps:\n",
        "        s, _ = env.reset()\n",
        "        seen_observations.append(s)\n",
        "        done = False\n",
        "\n",
        "        while not done and total_steps < max_steps:\n",
        "            s, r, done, _, _ = env.step(env.action_space.sample())\n",
        "            seen_observations.append(s)\n",
        "            total_steps += 1\n",
        "\n",
        "        if total_steps >= max_steps:\n",
        "            break\n",
        "\n",
        "    return seen_observations\n",
        "\n",
        "\n",
        "unwraped_env_samples = gather_samples(env)\n",
        "visualize_cartpole_observation_distribution(unwraped_env_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLejpiKKuU3T"
      },
      "source": [
        "## Discretize environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2triu6CuU3U"
      },
      "outputs": [],
      "source": [
        "from gymnasium.core import ObservationWrapper\n",
        "\n",
        "\n",
        "class Discretizer(ObservationWrapper):\n",
        "    def __init__(self, env, n_digits):\n",
        "        super().__init__(env)\n",
        "        self.n_digits = n_digits\n",
        "\n",
        "    def observation(self, state):\n",
        "        # Hint: you can do that with round(x, n_digits).\n",
        "        # You may pick a different n_digits for each dimension.\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        d_state = [np.round(val, self.n_digits) for val in state]\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return tuple(d_state)  # tuple to make it hashable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRvfLdyAuU3U",
        "outputId": "02b797c2-c53b-44c2-f328-883eb781ce01"
      },
      "outputs": [],
      "source": [
        "env = Discretizer(make_env(), n_digits=1)\n",
        "seen_observations = gather_samples(env)\n",
        "visualize_cartpole_observation_distribution(seen_observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU3tjzBsuU3V"
      },
      "source": [
        "## Learn discretized policy\n",
        "\n",
        "Now let's train a policy that uses discretized state space.\n",
        "\n",
        "__Tips:__\n",
        "\n",
        "* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n",
        "* If your discretization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce epsilon decay or change discretization. In practice we found that this kind of mistake is rather frequent.\n",
        "* If your discretization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n",
        "* **Start with a coarse discretization** and make it more fine-grained if that seems necessary.\n",
        "* Having $10^3$–$10^4$ distinct states is recommended (`len(agent._qvalues)`), but not required.\n",
        "* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n",
        "\n",
        "A reasonable agent should attain an average reward of at least 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV_NBzCquU3V",
        "outputId": "a28e8f3f-0462-4c04-f058-09008587d38c"
      },
      "outputs": [],
      "source": [
        "env = Discretizer(make_env(), n_digits=1)\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "\n",
        "rewards, epsilons = [], []\n",
        "seed_everything(env)\n",
        "\n",
        "for i in range(10000):\n",
        "    reward = play_and_train(env, agent)\n",
        "    rewards.append(reward)\n",
        "    epsilons.append(agent.epsilon)\n",
        "\n",
        "    #agent.epsilon *= 0.99\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        rewards_ewma = moving_average(rewards)\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.plot(rewards, label=\"rewards\")\n",
        "        plt.plot(rewards_ewma, label=\"rewards ewma@100\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.title(\"eps = {:e}, rewards ewma@100 = {:.1f}\".format(agent.epsilon, rewards_ewma[-1]))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZOXBC8MuU3W"
      },
      "source": [
        "## Homework Part I: On-policy learning and SARSA (3 points)\n",
        "\n",
        "The policy we're gonna use is epsilon-greedy policy, where agent takes the optimal action with probability $(1-\\epsilon)$, otherwise samples action at random. Note that agent __can__ occasionally sample optimal action during random sampling by pure chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyVHoPjuU3W"
      },
      "source": [
        "Now we gonna implement Expected Value SARSA on top of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYX_e5u1uU3X"
      },
      "outputs": [],
      "source": [
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \"\"\"\n",
        "    An agent that changes some of q-learning functions to implement Expected Value SARSA.\n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    \"\"\"\n",
        "        \n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        \n",
        "        state_value = self.get_qvalue(state, self.get_best_action(state))*(1-epsilon) + epsilon/len(possible_actions) * sum([self.get_qvalue(state, action) for action in possible_actions])\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0FdOJAPuU3Y"
      },
      "source": [
        "### Cliff World\n",
        "\n",
        "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
        "\n",
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n",
        "<center><i>Image from CS188</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xl3VqvTuU3Y"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CliffWalking-v1\", render_mode=\"rgb_array\")\n",
        "n_actions = env.action_space.n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_mXTPQguU3Y",
        "outputId": "8c1bf5ec-3595-40bd-9d8f-fca7dd1a308c"
      },
      "outputs": [],
      "source": [
        "# Our cliffworld has one difference from what's in the image: there is no wall.\n",
        "# Agent can choose to go as close to the cliff as it wishes.\n",
        "# x:start, T:exit, C:cliff, o: flat ground\n",
        "\n",
        "env.reset()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYQQgiuUuU3Z",
        "outputId": "a24b4a82-b371-47b6-8983-e0148abaa9dd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "seed_everything(env)\n",
        "\n",
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99, env=env)\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99, env=env)\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    # Note: agent.epsilon stays constant\n",
        "\n",
        "    if i % 250 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"EVSARSA mean reward =\", np.mean(rewards_sarsa[-100:]))\n",
        "        print(\"QLEARNING mean reward =\", np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label=\"ev_sarsa\")\n",
        "        plt.plot(moving_average(rewards_ql), label=\"qlearning\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1ipDWHuU3a"
      },
      "source": [
        "Let's now see what did the algorithms learn by visualizing their actions at every state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvohsxRouU3a"
      },
      "outputs": [],
      "source": [
        "def get_ascii_policy(agent):\n",
        "    \"\"\"Returns CliffWalkingEnv policy with arrows as a string. Hard-coded.\"\"\"\n",
        "\n",
        "    env = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
        "    env.reset()\n",
        "    grid = [x.split(\"  \") for x in env.render().split(\"\\n\")[:4]]\n",
        "\n",
        "    n_rows, n_cols = 4, 12\n",
        "    start_state_index = 36\n",
        "    actions = \"^>v<\"\n",
        "\n",
        "    policy_str = \"\"\n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if grid[yi][xi] == \"C\":\n",
        "                policy_str += \" C \"\n",
        "            elif (yi * n_cols + xi) == start_state_index:\n",
        "                policy_str += \" X \"\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                policy_str += \" T \"\n",
        "            else:\n",
        "                policy_str += \" %s \" % actions[agent.get_best_action(yi * n_cols + xi)]\n",
        "        policy_str += \"\\n\"\n",
        "\n",
        "    return policy_str\n",
        "\n",
        "\n",
        "def draw_policy(agent):\n",
        "    \"\"\"Prints CliffWalkingEnv policy with arrows.\"\"\"\n",
        "    print(get_ascii_policy(agent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW_NaXVyuU3a",
        "outputId": "e1804d7c-9455-48a8-b191-d99ba5a4e34d"
      },
      "outputs": [],
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(agent_sarsa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCL8o-iAuU3b"
      },
      "outputs": [],
      "source": [
        "sarsa_near_cliff_row = get_ascii_policy(agent_sarsa).strip().split(\"\\n\")[-2].strip().split()[1:-3]\n",
        "assert all(action == \"^\" for action in sarsa_near_cliff_row), (\n",
        "    \"SARSA policy should have '^' actions in the row near the cliff\"\n",
        ")\n",
        "\n",
        "qlearning_near_cliff_row = get_ascii_policy(agent_ql).strip().split(\"\\n\")[-2].strip().split()[1:-3]\n",
        "assert all(action == \">\" for action in qlearning_near_cliff_row), (\n",
        "    \"Q-learning policy should have '>' actions in the row near the cliff\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FcO9DJuU3b"
      },
      "source": [
        "## Expected Value SARSA for softmax policy (2 points)\n",
        "\n",
        "Implement an agent that would use a softmax policy for getting an action. Do not forget to also use softmax when calculating the expected value for value estimation. Draw the policy of the agent and see if the result is different compared to the previous approaches. Also, try using different temperatures ($\\tau$) and compare the results.\n",
        "\n",
        "$$ \\pi(a_i \\mid s) = \\operatorname{softmax} \\left( \\left\\{ {Q(s, a_j) \\over \\tau} \\right\\}_{j=1}^n \\right)_i = {\\operatorname{exp} \\left( Q(s,a_i) / \\tau \\right)  \\over {\\sum_{j}  \\operatorname{exp} \\left( Q(s,a_j) / \\tau  \\right)}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PncEvFb4uU3c"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "class SoftmaxEVSarsaAgent(EVSarsaAgent):\n",
        "    def __init__(self, alpha, tau, discount, env):\n",
        "        super().__init__(alpha, None, discount, env)\n",
        "        assert tau > 0\n",
        "        self.tau = tau\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Returns V_{pi} for current state under softmax policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        probs = np.array([np.exp(self.get_qvalue(state, action) /self.tau) for action in possible_actions])\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        value = sum(probs[i]*self.get_qvalue(state, action) for i, action in enumerate(possible_actions))\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.\n",
        "        We should take a random action with probability equaled softmax of q values.\n",
        "        \"\"\"\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # Tip: Use self.env.np_random.random() to generate a random number\n",
        "        # <YOUR CODE HERE>\n",
        "        probs = np.array([np.exp(self.get_qvalue(state, action) /self.tau) for action in possible_actions])\n",
        "        probs /= probs.sum()\n",
        "        \n",
        "        action = self.env.np_random.choice(possible_actions, p=probs)\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7EtndA8uU3e",
        "outputId": "83e09b95-3fa6-47bd-89fb-6ba9a7af7e18"
      },
      "outputs": [],
      "source": [
        "seed_everything(env)\n",
        "\n",
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99, env=env)\n",
        "\n",
        "agent_sarsa_softmax = SoftmaxEVSarsaAgent(alpha=0.25, tau=0.5, discount=0.99, env=env)\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99, env=env)\n",
        "\n",
        "rewards_sarsa, rewards_sarsa_softmax, rewards_ql = [], [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_sarsa_softmax.append(play_and_train(env, agent_sarsa_softmax))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "\n",
        "    if i % 250 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"EVSARSA mean reward =\", np.mean(rewards_sarsa[-100:]))\n",
        "        print(\"SOFTMAX_EVSARSA mean reward =\", np.mean(rewards_sarsa_softmax[-100:]))\n",
        "        print(\"QLEARNING mean reward =\", np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s, tau = %s\" % (agent_ql.epsilon, agent_sarsa_softmax.tau))\n",
        "        plt.plot(moving_average(rewards_sarsa), label=\"ev_sarsa\")\n",
        "        plt.plot(moving_average(rewards_sarsa_softmax), label=\"softmax_ev_sarsa\")\n",
        "        plt.plot(moving_average(rewards_ql), label=\"qlearning\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Z87r0auU3f",
        "outputId": "d7315704-2e86-4bc8-e04a-96e9949ff47f"
      },
      "outputs": [],
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(agent_sarsa)\n",
        "\n",
        "print(\"SOFTMAX SARSA\")\n",
        "draw_policy(agent_sarsa_softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGUK-UfTuU3g"
      },
      "outputs": [],
      "source": [
        "qlearning_near_cliff_row = get_ascii_policy(agent_sarsa_softmax).strip().split(\"\\n\")[-2].strip().split()[1:-3]\n",
        "assert all(action == \">\" for action in qlearning_near_cliff_row), (\n",
        "    \"Softmax SARSA policy should have '>' actions in the row near the cliff\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxpMIzc0uU3g"
      },
      "source": [
        "### More on SARSA\n",
        "\n",
        "Here are some of the things you can do if you feel like it:\n",
        "\n",
        "* Play with epsilon. See learned how policies change if you set epsilon to higher/lower values (e.g. 0.75).\n",
        "* Implement N-step algorithms and TD($\\lambda$): see [Sutton's book](http://incompleteideas.net/book/RLbook2020.pdf) chapter 7 and chapter 12.\n",
        "* Use those algorithms to train on CartPole in previous / next assignment for this week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMqUBNZtuU3h"
      },
      "source": [
        "## Part II: experience replay (2 points)\n",
        "\n",
        "There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: [spoiler] Experience replay :)\n",
        "\n",
        "The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n",
        "\n",
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=480>\n",
        "\n",
        "#### Training with experience replay\n",
        "1. Play game, sample `<s,a,r,s'>`.\n",
        "2. Update q-values based on `<s,a,r,s'>`.\n",
        "3. Store `<s,a,r,s'>` transition in a buffer.\n",
        " 3. If buffer is full, delete earliest data.\n",
        "4. Sample K such transitions from that buffer and update q-values based on them.\n",
        "\n",
        "\n",
        "To enable such training, first we must implement a memory structure that would act like such a buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKxAnsDQuU3i"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GNFzkSjeuU3i"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "\n",
        "        Note: for this assignment you can pick any data structure you want.\n",
        "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
        "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "\n",
        "        # OPTIONAL: YOUR CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        \"\"\"\n",
        "        Make sure, _storage will not exceed _maxsize.\n",
        "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
        "        \"\"\"\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        # add data to storage\n",
        "        # <YOUR CODE HERE>\n",
        "        self._storage.append(data)\n",
        "        if len(self._storage) > self._maxsize:\n",
        "            self._storage.pop(0)\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        storage = np.array(self._storage)\n",
        "        # <YOUR CODE HERE>\n",
        "        idxes = np.random.randint(0, len(self), size=batch_size)\n",
        "        # collect <s,a,r,s',done> for each index\n",
        "        return (\n",
        "            np.array( storage[idxes, 0] ),\n",
        "            np.array( storage[idxes, 1] ),\n",
        "            np.array( storage[idxes, 2] ),\n",
        "            np.array( storage[idxes, 3] ),\n",
        "            np.array( storage[idxes, 4] ),\n",
        "        )\n",
        "\n",
        "        # </END OF YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZoUr2j5uU3j"
      },
      "source": [
        "Some tests to make sure your buffer works right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlCCtFQPuU3t",
        "outputId": "fd51fa35-29c6-474d-be70-94edefc63006"
      },
      "outputs": [],
      "source": [
        "def obj2arrays(obj):\n",
        "    for x in obj:\n",
        "        yield np.array([x])\n",
        "\n",
        "\n",
        "def obj2sampled(obj):\n",
        "    return tuple(obj2arrays(obj))\n",
        "\n",
        "\n",
        "replay = ReplayBuffer(2)\n",
        "obj1 = (0, 1, 2, 3, True)\n",
        "obj2 = (4, 5, 6, 7, False)\n",
        "replay.add(*obj1)\n",
        "assert replay.sample(1) == obj2sampled(obj1), (\n",
        "    \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
        ")\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"Please make sure __len__ methods works as intended.\"\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj2)\n",
        "replay.add(*obj1)\n",
        "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
        "replay.add(*obj1)\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj1)\n",
        "print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJhukbhVuU3u"
      },
      "source": [
        "Now let's use this buffer to improve training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1guraA_UuU3u"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "# env = Discretizer(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), n_digits=2)\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQcK5puTuU3v"
      },
      "outputs": [],
      "source": [
        "def play_and_train_with_replay(env, agent, replay=None, t_max=10**4, replay_batch_size=32):\n",
        "    \"\"\"\n",
        "    This function should\n",
        "    - run a full game, actions given by agent.get_action(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\n",
        "    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n",
        "        If None, do not use experience replay\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s\n",
        "        # <YOUR CODE HERE>\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        next_s, r, done, trunc, _ = env.step(a)\n",
        "\n",
        "        # update agent on current transition. Use agent.update\n",
        "        # <YOUR CODE HERE>\n",
        "        agent.update(s, a, r, next_s)\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        if replay is not None:\n",
        "            # store current <s,a,r,s'> transition in buffer\n",
        "            # <YOUR CODE HERE>\n",
        "            replay.add(s, a, r, next_s, done)\n",
        "            # </END OF YOUR CODE>\n",
        "\n",
        "            # sample replay_batch_size random transitions from replay,\n",
        "            # then update agent on each of them in a loop\n",
        "            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n",
        "            for i in range(replay_batch_size):\n",
        "                # <YOUR CODE HERE>\n",
        "                agent.update(s_[i], a_[i], r_[i], next_s_[i], done_[i])\n",
        "\n",
        "                # </END OF YOUR CODE>\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivYL4MoNuU3v"
      },
      "outputs": [],
      "source": [
        "# Create two agents: first will use experience replay, second will not.\n",
        "\n",
        "agent_baseline = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "\n",
        "agent_replay = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "\n",
        "replay = ReplayBuffer(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg-AKBOYuU3w",
        "outputId": "150dda93-ad47-4996-ec4d-1f9ed3fa4445"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({\"x\": np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "\n",
        "rewards_replay, rewards_baseline = [], []\n",
        "\n",
        "for i in range(1000):\n",
        "    rewards_replay.append(play_and_train_with_replay(env, agent_replay, replay))\n",
        "    rewards_baseline.append(play_and_train_with_replay(env, agent_baseline, replay=None))\n",
        "\n",
        "    agent_replay.epsilon *= 0.99\n",
        "    agent_baseline.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\n",
        "            \"Baseline : eps =\",\n",
        "            agent_replay.epsilon,\n",
        "            \"mean reward =\",\n",
        "            np.mean(rewards_baseline[-10:]),\n",
        "        )\n",
        "        print(\n",
        "            \"ExpReplay: eps =\",\n",
        "            agent_baseline.epsilon,\n",
        "            \"mean reward =\",\n",
        "            np.mean(rewards_replay[-10:]),\n",
        "        )\n",
        "        plt.plot(moving_average(rewards_replay), label=\"exp. replay\")\n",
        "        plt.plot(moving_average(rewards_baseline), label=\"baseline\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.array(rewards_replay[:200]).mean() > np.array(rewards_baseline[:200]).mean(), (\n",
        "    \"Experienced replay should improve performance\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_49Ijv1zuU3x"
      },
      "source": [
        "#### What to expect:\n",
        "\n",
        "Experience replay, if implemented correctly, will improve algorithm's initial convergence a lot, but it shouldn't affect the final performance.\n",
        "\n",
        "If you're feeling that you need more examples to understand how experience replay works, try using it for discretized state spaces (CartPole or other __[classic control envs](https://gym.openai.com/envs/#classic_control)__)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5nanIJnuU3y"
      },
      "source": [
        "# N-step TD learning\n",
        "\n",
        "In this part, we will get acquainted with multi-step estimates. We will start with an on-policy setting, but as a bonus you can also implement the off-policy version.\n",
        "\n",
        "Recall that TD(0) estimates have low variance but high bias, while full trajectory estimates have low bias but high variance. For many tasks, the optimal approach lies somewhere in between these extremes. N-step methods provide this middle ground by allowing us to control the bias-variance tradeoff through the choice of n.\n",
        "\n",
        "**Practical note:** Analyzing performance with different n-step values can be difficult due to noisy rewards. In that case, use the benchmark_agents function, which can visualize multi-seed performance.\n",
        "\n",
        "## N-step EV-SARSA (2 points)\n",
        "\n",
        "Look at section 7.2 in Sutton Barto book: http://incompleteideas.net/book/the-book-2nd.html\n",
        "\n",
        "Regular n-step TD (without episodic buffer):\n",
        "\n",
        "For each step $t$ we take the future window $[t,t+n]$ (if available) and calculate the return:\n",
        "\n",
        "$$G_t = r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{n-1} r_{t+n-1} + \\gamma^n V(s_{t+n})$$\n",
        "\n",
        "This is done from left to right (as data accumulates). If the episode ends earlier, we take the actual length of the remaining trajectory.\n",
        "\n",
        "So,\n",
        "\n",
        "---\n",
        "### n-step Sarsa for estimating $Q \\approx q_* \\text{ or } q_\\pi$\n",
        "\n",
        "- Initialize $Q(s,a)$ arbitrarily, for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$.\n",
        "- Initialize $\\pi$ to be $\\varepsilon$-greedy with respect to $Q$, or to a fixed given policy.\n",
        "- Algorithm parameters: step size $\\alpha \\in (0,1]$, small $\\varepsilon > 0$, a positive integer $n$.\n",
        "- All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$.\n",
        "\n",
        "For each episode:\n",
        "- Initialize and store $S_0 \\neq$ terminal.\n",
        "- Select and store an action $A_0 \\sim \\pi(\\cdot|S_0)$.\n",
        "- $T \\leftarrow \\infty$\n",
        "\n",
        "For $t = 0,1,2,\\dots$:\n",
        "- If $t < T$:\n",
        "  - Take action $A_t$.\n",
        "  - Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$.\n",
        "  - If $S_{t+1}$ is terminal, set $T \\leftarrow t + 1$.\n",
        "  - Else, select and store an action $A_{t+1} \\sim \\pi(\\cdot|S_{t+1})$.\n",
        "\n",
        "- $\\tau \\leftarrow t - n + 1$ ($\\tau$ is the time whose estimate is being updated).\n",
        "\n",
        "- If $\\tau \\geq 0$:\n",
        "  - $G \\leftarrow \\sum_{i=\\tau+1}^{\\min(\\tau+n,T)} \\gamma^{i-\\tau-1} R_i$.\n",
        "  - If $\\tau + n < T$, then $G \\leftarrow G + \\gamma^n Q(S_{\\tau+n}, A_{\\tau+n}) \\quad (G_{\\tau:\\tau+n})$.\n",
        "  - $Q(S_\\tau,A_\\tau) \\leftarrow Q(S_\\tau,A_\\tau) + \\alpha[G - Q(S_\\tau,A_\\tau)]$.\n",
        "  - If $\\pi$ is being learned, ensure $\\pi(\\cdot|S_\\tau)$ is $\\varepsilon$-greedy wrt $Q$.\n",
        "\n",
        "- If $\\tau = T - 1$, break.\n",
        "\n",
        "---\n",
        "\n",
        "Since we are working with EVSARSA, we will have a small difference:\n",
        "\n",
        "It consists of a linear string of sample actions and states, just as in *n*-step Sarsa, except that its last element is a branch over all action possibilities weighted, as always, by their probability under $\\pi$. This algorithm can be described by the same equation as *n*-step Sarsa (above) except with the *n*-step return redefined as\n",
        "\n",
        "$$\n",
        "G_{t:t+n} \\doteq R_{t+1} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\bar{V}_{t+n-1}(S_{t+n}),\n",
        "\\quad t + n < T,\n",
        "$$\n",
        "\n",
        "(with $ G_{t:t+n} \\doteq G_t $ for $ t + n \\geq T $ where $ \\bar{V}_t(s) $ is the *expected approximate value* of state $ s $, using the estimated action values at time $ t $, under the target policy:\n",
        "\n",
        "$$\n",
        "\\bar{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a), \\quad \\text{for all } s \\in \\mathcal{S}.\n",
        "$$\n",
        "\n",
        "Expected approximate values are used in developing many of the action-value methods in the rest of this book. If $ s $ is terminal, then its expected approximate value is defined to be 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOLCjb7quU3z"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class NStepEVSarsaAgent(EVSarsaAgent):\n",
        "    def __init__(self, n_steps, *args, **kwargs):\n",
        "        assert n_steps >= 1, \"n_step must be greater than or equal to 1\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.n_steps = n_steps\n",
        "        self.t = 0  # aka current step\n",
        "        self.T = sys.maxsize  # aka terminal step\n",
        "        self.memory = []\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        if len(self.memory) == 0:\n",
        "            self.memory.append(Transition(state, action, reward, next_state, done))\n",
        "            return\n",
        "\n",
        "        if self.t < self.T:\n",
        "            self.memory.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "            if done:\n",
        "                self.T = self.t + 1\n",
        "\n",
        "        tau = self.t - self.n_steps\n",
        "        if tau >= 0:\n",
        "            start_index, end_index = tau + 1, min(tau + self.n_steps, self.T)\n",
        "            G = 0\n",
        "            # <YOUR CODE HERE>\n",
        "            G = sum(self.discount**(i-tau-1) * self.memory[i % self.n_steps].reward for i in range(start_index, end_index+1))                \n",
        "\n",
        "            # </END OF YOUR CODE>\n",
        "\n",
        "            if tau + self.n_steps < self.T:\n",
        "                next_s = self.memory[tau + self.n_steps].next_state\n",
        "                if next_s is not None:\n",
        "                    # <YOUR CODE HERE>\n",
        "                    G = G + self.discount**self.n_steps * self.get_value(next_s)\n",
        "                    # </END OF YOUR CODE>\n",
        "\n",
        "            state_tau, action_tau = (\n",
        "                self.memory[tau].state,\n",
        "                self.memory[tau].action,\n",
        "            )  # state and action to be updated\n",
        "\n",
        "            # <YOUR CODE HERE>\n",
        "            new_qvalue = self.get_qvalue(state_tau, action_tau)*(1-self.alpha) + self.alpha*G\n",
        "\n",
        "            # </END OF YOUR CODE>\n",
        "            self.set_qvalue(state_tau, action_tau, new_qvalue)\n",
        "        self.t += 1\n",
        "\n",
        "        if self.t >= self.T and tau < self.T - 1:\n",
        "            self.update(self.memory[self.T - 1].state, self.memory[self.T - 1].action, 0, None, True)\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory = []\n",
        "        self.t = 0\n",
        "        self.T = sys.maxsize\n",
        "\n",
        "\n",
        "def play_and_train_nstep(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should\n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "    agent.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, terminated, truncated, info = env.step(a)\n",
        "        done = terminated\n",
        "\n",
        "        # train (update) agent for state s\n",
        "        agent.update(s, a, r, next_s, done)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            agent.reset()\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrRAa2g2uU3z"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def evaluate_nstep_evsarsa(env_builder, n_episodes=1000, t_max=10000, n_seeds=3, span=10):\n",
        "    params = {\"alpha\": 0.1, \"epsilon\": 0.1, \"discount\": 0.99, \"env\": env_builder()}\n",
        "\n",
        "    exp_setups = [\n",
        "        {\n",
        "            \"name\": f\"nstep_evsarsa_{n_step}\",\n",
        "            \"agent_builder\": partial(NStepEVSarsaAgent, n_steps=n_step, **params),\n",
        "            \"env\": env_builder,\n",
        "            \"train_foo\": play_and_train_nstep,\n",
        "        }\n",
        "        for n_step in [1, 2, 4]\n",
        "    ]\n",
        "    _ = benchmark_agents(\n",
        "        exp_setups, num_episodes=n_episodes, t_max=t_max, plot_every=1000, span=span, num_seeds=n_seeds\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xim21jczuU30",
        "outputId": "472dbaac-36d6-4057-80d1-8612db85f2f4"
      },
      "outputs": [],
      "source": [
        "evaluate_nstep_evsarsa(lambda: gym.make(\"CliffWalking-v1\"), n_episodes=50, t_max=1000, n_seeds=5, span=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm2rWuDHuU30",
        "outputId": "f8ae49cf-9dee-41d1-f77d-9bdd00f95346"
      },
      "outputs": [],
      "source": [
        "evaluate_nstep_evsarsa(lambda: Discretizer(gym.make(\"MountainCar-v0\"), n_digits=2), n_episodes=200, n_seeds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M-1rdPluU31"
      },
      "source": [
        "## Eligibility traces: (1 point)\n",
        "\n",
        "Eligibility traces are a mechanism that helps to assign credit to states and actions that occurred in the past, allowing for more efficient learning by updating multiple state-action pairs in a single step.\n",
        "\n",
        "Typically, eligibility traces are implemented as decaying values for each state-action pair, with recent pairs having higher trace values than older ones. At the beginning of each episode, all eligibility traces are initialized to zero, and at the end of an episode, they are reset to prepare for the next learning sequence.\n",
        "\n",
        "During each step, eligibility traces are updated by incrementing the value for the current state-action pair and applying a decay factor to all traces, allowing the algorithm to update all previously visited state-action pairs proportionally to their recency and relevance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Watkins($\\lambda$) algorithm\n",
        "**Initialize** $ Q(s, a) $ arbitrarily and $ e(s, a) = 0 $, for all $ s, a $.\n",
        "\n",
        "**Repeat** (for each episode):\n",
        "\n",
        "1. Initialize $ s, a $.\n",
        "2. **Repeat** (for each step of episode):\n",
        "   - Take action $ a $, observe $ r, s' $.\n",
        "   - Choose $ a' $ from $ s' $ using policy derived from $ Q $ (e.g., $ \\varepsilon $-greedy).\n",
        "   - $ a^* \\leftarrow \\arg\\max_b Q(s', b) $ (if $ a' $ ties for the max, then $ a^* \\leftarrow a' $).\n",
        "   - $ \\delta \\leftarrow r + \\gamma Q(s', a^*) - Q(s, a) $.\n",
        "   - $ e(s, a) \\leftarrow e(s, a) + 1 $.\n",
        "   - **For all** $ s, a $:\n",
        "     - $ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta e(s, a) $.\n",
        "     - If $ a' = a^* $, then $ e(s, a) \\leftarrow \\gamma \\lambda e(s, a) $,  \n",
        "       else $ e(s, a) \\leftarrow 0 $.\n",
        "   - $ s \\leftarrow s' $; $ a \\leftarrow a' $.\n",
        "\n",
        "**until** $ s $ is terminal.\n",
        "\n",
        "---\n",
        "\n",
        "We have two policies: the target policy (greedy) and the behavior policy (ε-greedy).\n",
        "\n",
        "Watkins' Q(λ) algorithm addresses the challenge of using eligibility traces in off-policy learning. The key insight is that we can only reliably update state-action pairs along a trajectory up to the point where we take a non-greedy action.\n",
        "\n",
        "When we select an action a' that is also the greedy action a* (meaning our behavior and target policies align), we can continue propagating updates through the eligibility traces, decaying them by γλ as usual.\n",
        "\n",
        "However, when we select a non-greedy action (a' ≠ a*), we must cut the eligibility traces to zero. This is because the future trajectory will follow the behavior policy, not the target policy we're trying to learn.\n",
        "\n",
        "This approach ensures that Q-values are only updated based on sequences where the behavior matched the target policy, maintaining the validity of the off-policy updates while still benefiting from eligibility traces when possible.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bXbBxJfDuU31"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class QLambdaAgent(QLearningAgent):\n",
        "    def __init__(self, alpha, epsilon, discount, lambda_factor, env):\n",
        "        assert 0 <= lambda_factor <= 1, \"lambda must be between 0 and 1\"\n",
        "        super().__init__(alpha, epsilon, discount, env)\n",
        "        self.eligibility_trace = defaultdict(lambda: 0)\n",
        "        self.lambda_factor = lambda_factor\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def update(self, state, action, reward, next_state, _done):\n",
        "        \"\"\"\n",
        "        Implements Q(λ) update rule:\n",
        "        - Updates Q-values using eligibility traces.\n",
        "        - Adjusts traces based on whether the selected action matches the best action.\n",
        "        \"\"\"\n",
        "        # Update eligibility traces\n",
        "        for s, a in list(self.eligibility_trace.keys()):\n",
        "            if a == self.get_best_action(s):\n",
        "                # Decay eligibility trace (but never immediately zero it)\n",
        "                # <YOUR CODE HERE>\n",
        "                self.eligibility_trace[(s, a)] *= self.discount*self.lambda_factor\n",
        "                # </END OF YOUR CODE>\n",
        "            else:\n",
        "                # <YOUR CODE HERE>\n",
        "                self.eligibility_trace[(s, a)] = 0\n",
        "                # </END OF YOUR CODE>\n",
        "\n",
        "            # Remove traces close to zero\n",
        "            if abs(self.eligibility_trace[(s, a)]) < 1e-6:\n",
        "                del self.eligibility_trace[(s, a)]\n",
        "\n",
        "        # Compute TD error with check for terminal state\n",
        "        next_value = self.get_value(next_state) if next_state is not None else 0\n",
        "        delta = reward + self.discount * next_value - self.get_qvalue(state, action)\n",
        "\n",
        "        # Update eligibility trace for (state, action)\n",
        "        # <YOUR CODE HERE>\n",
        "        self.eligibility_trace[(state, action)] += 1\n",
        "\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        # Iterate over all stored (s, a) pairs in eligibility trace\n",
        "        for s, a in list(self.eligibility_trace.keys()):\n",
        "            # <YOUR CODE HERE>\n",
        "            new_qvalue = self.get_qvalue(s, a) + self.alpha*delta*self.eligibility_trace[(s, a)]\n",
        "\n",
        "            # </END OF YOUR CODE>\n",
        "            self.set_qvalue(s, a, new_qvalue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CbKaugVkuU3-"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def evaluate_lambda_qlearning(env_builder, n_episodes=1000, t_max=10000, n_seeds=3, span=10):\n",
        "    params = {\"alpha\": 0.1, \"epsilon\": 0.1, \"discount\": 0.99, \"env\": env_builder()}\n",
        "    exp_setups = []\n",
        "    exp_setups.extend(\n",
        "        [\n",
        "            {\n",
        "                \"name\": f\"lambda_qlearning_{lambda_factor}\",\n",
        "                \"agent_builder\": partial(QLambdaAgent, lambda_factor=lambda_factor, **params),\n",
        "                \"env\": env_builder,\n",
        "                \"train_foo\": play_and_train_nstep,\n",
        "            }\n",
        "            for lambda_factor in [0.0, 0.2, 0.5, 0.95]\n",
        "        ]\n",
        "    )\n",
        "    _ = benchmark_agents(\n",
        "        exp_setups, num_episodes=n_episodes, t_max=t_max, plot_every=1000, span=span, num_seeds=n_seeds\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VqdutufbuU3-",
        "outputId": "d69c42d3-ef37-4d4b-89f2-c677cab83ac9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'play_and_train_nstep' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_lambda_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCliffWalking-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_seeds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mevaluate_lambda_qlearning\u001b[39m\u001b[34m(env_builder, n_episodes, t_max, n_seeds, span)\u001b[39m\n\u001b[32m      5\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mepsilon\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdiscount\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.99\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m: env_builder()}\n\u001b[32m      6\u001b[39m exp_setups = []\n\u001b[32m      7\u001b[39m exp_setups.extend(\n\u001b[32m      8\u001b[39m     [\n\u001b[32m      9\u001b[39m         {\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlambda_qlearning_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33magent_builder\u001b[39m\u001b[33m\"\u001b[39m: partial(QLambdaAgent, lambda_factor=lambda_factor, **params),\n\u001b[32m     12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m: env_builder,\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtrain_foo\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mplay_and_train_nstep\u001b[49m,\n\u001b[32m     14\u001b[39m         }\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m lambda_factor \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.0\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.95\u001b[39m]\n\u001b[32m     16\u001b[39m     ]\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m _ = benchmark_agents(\n\u001b[32m     19\u001b[39m     exp_setups, num_episodes=n_episodes, t_max=t_max, plot_every=\u001b[32m1000\u001b[39m, span=span, num_seeds=n_seeds\n\u001b[32m     20\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'play_and_train_nstep' is not defined"
          ]
        }
      ],
      "source": [
        "evaluate_lambda_qlearning(lambda: gym.make(\"CliffWalking-v1\"), n_episodes=150, t_max=1000, n_seeds=5, span=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m12E5zX6uU3_",
        "outputId": "05eea3af-705f-4a4f-8acf-43ac78b20f02"
      },
      "outputs": [],
      "source": [
        "evaluate_lambda_qlearning(lambda: gym.make(\"Taxi-v3\"), n_episodes=150, t_max=1000, n_seeds=5, span=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kYmsoXwuU4B",
        "outputId": "7859c111-3baa-4633-a74f-e1c3a9a55a94"
      },
      "outputs": [],
      "source": [
        "evaluate_lambda_qlearning(lambda: Discretizer(gym.make(\"MountainCar-v0\"), n_digits=2), n_episodes=200, n_seeds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIVgaxIZuU4C"
      },
      "source": [
        "## N-step estimates in off-policy setting. Retrace algorithm. (3-5 points)\n",
        "\n",
        "Implement Retrace algorithm and use it with Replay Buffer\n",
        "\n",
        "## Understanding Retrace(λ)\n",
        "\n",
        "Retrace(λ) is an off-policy reinforcement learning algorithm that combines the benefits of eligibility traces with importance sampling to provide stable and efficient learning from off-policy data.\n",
        "\n",
        "### Key Features of Retrace\n",
        "\n",
        "1. **Off-policy Learning**: Retrace can learn from data generated by any behavior policy, making it suitable for experience replay.\n",
        "\n",
        "2. **Eligibility Traces**: Like TD(λ), Retrace uses eligibility traces to credit past state-action pairs, allowing for faster learning.\n",
        "\n",
        "3. **Truncated Importance Sampling**: The key innovation in Retrace is the use of truncated importance sampling ratios ($\\min(1, \\frac{\\pi(a|s)}{\\mu(a|s)})$), which ensures the product of these ratios doesn't grow too large and cause variance issues.\n",
        "\n",
        "### The Retrace Update Rule\n",
        "\n",
        "Retrace maintains a running return estimate $Q^{ret}$ defined recursively:\n",
        "\n",
        "$$Q^{ret}(s_t, a_t) = r_t + \\gamma[(1-\\alpha)Q(s_{t+1}, a_{t+1}) + \\alpha(Q(s_{t+1}, a_{t+1}) + c_{t+1}\\lambda\\delta_{t+1})]$$\n",
        "\n",
        "Where:\n",
        "- $c_{t+1} = \\min(1, \\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})})$ is the truncated importance sampling ratio\n",
        "- $\\delta_{t+1} = Q^{ret}(s_{t+1}, a_{t+1}) - Q(s_{t+1}, a_{t+1})$ is the TD error for the next state-action pair\n",
        "- $\\lambda$ is the trace decay parameter\n",
        "\n",
        "This update effectively combines:\n",
        "- Q-learning's off-policy capability\n",
        "- SARSA's ability to use eligibility traces\n",
        "- Stability through truncated importance sampling\n",
        "\n",
        "### Advantages Over Traditional Methods\n",
        "\n",
        "1. **Compared to Q-learning**: Retrace incorporates multi-step returns through eligibility traces, which can lead to faster convergence in many environments.\n",
        "\n",
        "2. **Compared to SARSA(λ)**: Retrace is off-policy, so it can learn from exploratory actions without being affected by them.\n",
        "\n",
        "3. **Compared to Importance Sampling Methods**: By truncating the importance ratios, Retrace avoids the high variance issues that plague many importance sampling approaches.\n",
        "\n",
        "### Applications\n",
        "\n",
        "Retrace is particularly well-suited for:\n",
        "- Experience replay settings\n",
        "- Learning from human demonstrations\n",
        "- Any setting where on-policy data collection is expensive or impractical\n",
        "\n",
        "## Task\n",
        "\n",
        "Compare different Retrace methods: without Importance Sampling, with Importance Sampling, and with Retrace. Ensure the differences are noticeable. Test different training settings, such as collecting trajectories with one agent and using them to initialize Retrace. Collect several episodes using a uniform policy and use them to initialize the buffer. Analyze performance and convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxW_Lqb6uU4E"
      },
      "outputs": [],
      "source": [
        "def play_and_train_retrace(env, agent, replay, t_max, n_train_steps, trajectory_len):\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "    replay._episodes.append([])\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # Get action from agent\n",
        "        a = agent.get_action(s)\n",
        "        prob = agent.get_action_probability(s, a)\n",
        "\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        replay._episodes[-1].append((s, a, r, next_s, done, prob))\n",
        "\n",
        "        # Update state and total reward\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "\n",
        "        if replay is not None and len(replay) > 0:\n",
        "            for _ in range(n_train_steps):\n",
        "                trajectory = replay.sample(max_trajectory_len=trajectory_len)  # whole trajectory\n",
        "                agent.retrace_update(trajectory)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYNBDQ-nuU4E"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "\n",
        "class SequenceBuffer:\n",
        "    def __init__(self, episode_capacity):\n",
        "        self._episodes = deque(maxlen=episode_capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._episodes)\n",
        "\n",
        "    def add_sequence(self, sequence):\n",
        "        self._episodes.append(sequence)\n",
        "\n",
        "    def sample(self, max_trajectory_len=None):\n",
        "        seq_idx = np.random.randint(0, len(self))\n",
        "        sequence = self._episodes[seq_idx]\n",
        "\n",
        "        if max_trajectory_len is None:\n",
        "            return sequence\n",
        "\n",
        "        start_idx = np.random.randint(0, max(len(sequence) - max_trajectory_len, 1))\n",
        "        return sequence[start_idx : start_idx + max_trajectory_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iSX3VsouU4F"
      },
      "outputs": [],
      "source": [
        "class RetraceAgent(QLearningAgent):\n",
        "    def __init__(self, alpha, epsilon, discount, env, lambda_factor=0.9):\n",
        "        super().__init__(alpha, epsilon, discount, env)\n",
        "        self.lambda_factor = lambda_factor\n",
        "\n",
        "    def get_action_probability(self, state, action):\n",
        "        # <YOUR CODE HERE>\n",
        "        N_actions = len(self.get_legal_actions(state))\n",
        "        return 1-self.epsilon+self.epsilon/N_actions if self.get_best_action(state) == action else self.epsilon/N_actions\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "    def retrace_update(self, trajectory):\n",
        "        last_state, last_action, last_reward, last_next_state, last_done, last_behavior_prob = trajectory[-1]\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        G = last_reward\n",
        "        if not last_done:\n",
        "            G += self.discount * self.get_value(last_next_state)\n",
        "        # </END OF YOUR CODE>\n",
        "\n",
        "        for state, action, reward, next_state, done, behavior_prob in reversed(trajectory[:-1]):\n",
        "            # <YOUR CODE HERE>\n",
        "            pi_prob = 1 if self.get_best_action(state)==action else 0\n",
        "            c = min(1.0,  pi_prob / behavior_prob) if behavior_prob > 0 else 0.0\n",
        "            # </END OF YOUR CODE>\n",
        "            \n",
        "            # <YOUR CODE HERE>\n",
        "            expected_Q = self.get_value(next_state) if not done else 0\n",
        "            # </END OF YOUR CODE>\n",
        "\n",
        "            # <YOUR CODE HERE>\n",
        "            G = reward + self.discount * (expected_Q + \\\n",
        "                            self.alpha*self.lambda_factor * c * (G - self.get_qvalue(state, action)))\n",
        "            # </END OF YOUR CODE>\n",
        "\n",
        "            # <YOUR CODE HERE>\n",
        "            current_q = self.get_qvalue(state, action)\n",
        "            new_q_value = current_q + self.alpha * (G - current_q)\n",
        "            # </END OF YOUR CODE>\n",
        "            self.set_qvalue(state, action, new_q_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16Y2ol2luU4F",
        "outputId": "388d9b9d-f31e-40f4-c1a4-8b40077dbfd8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ReplayBuffer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m agent_baseline = QLearningAgent(alpha=\u001b[32m0.5\u001b[39m, epsilon=\u001b[32m0.25\u001b[39m, discount=\u001b[32m0.99\u001b[39m, env=env)\n\u001b[32m      5\u001b[39m agent_replay = QLearningAgent(alpha=\u001b[32m0.5\u001b[39m, epsilon=\u001b[32m0.25\u001b[39m, discount=\u001b[32m0.99\u001b[39m, env=env)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m replay = \u001b[43mReplayBuffer\u001b[49m(\u001b[32m1000\u001b[39m)\n\u001b[32m      8\u001b[39m sequence_buffer = SequenceBuffer(\u001b[32m1000\u001b[39m)\n\u001b[32m      9\u001b[39m retrace_agent = RetraceAgent(alpha=\u001b[32m0.25\u001b[39m, epsilon=\u001b[32m0.25\u001b[39m, discount=\u001b[32m0.99\u001b[39m, env=env, lambda_factor=\u001b[32m0.9\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'ReplayBuffer' is not defined"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CliffWalking-v1\")\n",
        "\n",
        "agent_baseline = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "\n",
        "agent_replay = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, env=env)\n",
        "replay = ReplayBuffer(1000)\n",
        "\n",
        "sequence_buffer = SequenceBuffer(1000)\n",
        "retrace_agent = RetraceAgent(alpha=0.25, epsilon=0.25, discount=0.99, env=env, lambda_factor=0.9)\n",
        "\n",
        "rewards_replay, rewards_baseline, rewards_retrace = [], [], []\n",
        "\n",
        "t_max = 100\n",
        "for i in trange(300):\n",
        "    rewards_replay.append(play_and_train_with_replay(env, agent_replay, replay, t_max=t_max))\n",
        "    rewards_baseline.append(play_and_train_with_replay(env, agent_baseline, replay=None, t_max=t_max))\n",
        "    rewards_retrace.append(\n",
        "        play_and_train_retrace(env, retrace_agent, sequence_buffer, t_max=t_max, trajectory_len=8, n_train_steps=32)\n",
        "    )\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"Baseline : eps =\", agent_replay.epsilon, \"mean reward =\", np.mean(rewards_baseline[-10:]))\n",
        "        print(\"ExpReplay: eps =\", agent_baseline.epsilon, \"mean reward =\", np.mean(rewards_replay[-10:]))\n",
        "        print(\"Retrace: eps =\", retrace_agent.epsilon, \"mean reward =\", np.mean(rewards_retrace[-10:]))\n",
        "        plt.plot(moving_average(rewards_replay, span=50), label=\"exp. replay\")\n",
        "        plt.plot(moving_average(rewards_baseline, span=50), label=\"baseline\")\n",
        "        plt.plot(moving_average(rewards_retrace, span=50), label=\"retrace\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VkrRpvWuU4G"
      },
      "source": [
        "**Bonus:** You can also analyze the performance of different agents with different hyperparameters across various environments. For the N-step bonus, it would be interesting to show how the variance and bias of Q-values depend on nn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgfHKwkLuU4H"
      },
      "source": [
        "\n",
        "### Outro\n",
        "\n",
        "We will use the code you just wrote extensively in the next week of our course.\n",
        "\n",
        "__Next week__ we're gonna explore how q-learning and similar algorithms can be applied for large state spaces, with deep learning models to approximate the Q function.\n",
        "\n",
        "However, __the code you've written__ for this week is already capable of solving many RL problems, and as an added benefit - it is very easy to detach. You can use Q-learning, SARSA and Experience Replay for any RL problems you want to solve - just thow 'em into a file and import the stuff you need."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
